{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "  - K-Nearest Neighbors (KNN) is a simple, instance-based (lazy) machine learning algorithm used for both classification and regression.\n",
        "  - It doesn’t build an explicit model during training—instead, it stores the entire dataset and makes predictions by looking at the K closest data points to a new input."
      ],
      "metadata": {
        "id": "52wgBnBMCaSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "   - The Curse of Dimensionality refers to a set of problems that arise when the number of features (dimensions) in a dataset becomes very large. As dimensionality increases, data becomes sparse, and many machine-learning algorithms—especially distance-based ones like KNN—start to perform poorly.\n"
      ],
      "metadata": {
        "id": "VU48tnvOC0KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "  - Principal Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much information (variance) as possible.\n",
        "   - It does this by transforming the original features into a new set of uncorrelated variables called principal components."
      ],
      "metadata": {
        "id": "4zg-r4ZcDEp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "   - In Principal Component Analysis (PCA), eigenvalues and eigenvectors come from the covariance matrix (or correlation matrix) of the data, and they are the mathematical heart of PCA. They tell us which directions matter most and how much information each direction carries."
      ],
      "metadata": {
        "id": "cOQQVRETFE89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "   - KNN and PCA work really well together because PCA fixes many of KNN’s weaknesses. In a single pipeline, PCA is typically applied before KNN, and the combination improves accuracy, speed, and stability.\n"
      ],
      "metadata": {
        "id": "HgIU-AnBFVsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nQtwwWPMFqpg"
      }
    }
  ]
}